---
title: 熵值法用于特征剔除
toc: true
date: 2019-08-16 12:35:01
tags: [数学建模,机器学习]
categories:
- 机器学习
---

熵是对不确定性的一种度量，信息量越大，不确定性越小，熵越小。当用熵值判断某个指标的离散程度，指标的离散程度越大，该指标对综合评价的影响越大。

**熵值越小，指标的离散程度越大，该指标对综合评价的影响 (即权重) 越大。**

<!--more-->

## 问题背景

mcm2012A 葡萄酒评价，给出了葡萄酒的大量指标，希望将影响葡萄酒质量的指标数量减小。

## 步骤

### step1:

初始数据矩阵$x = (x_{ij})_{m \times n}$，表示第i号葡萄酒的第j项指标的数值，先将各指标归一化处理，然后计算$x_{ij}$在第j项指标的比重$p_{ij}$:

$$p_{ij} = x_{ij}/\sum_{i=1}^m x_{ij}$$

### step2:

计算第j各指标的熵值$e_{j}$:

$$e_{j} = -k\sum_{i=1}^mp_{ij} \ln p_{ij}$$

其中 k > 0 , $e_j>=0$,当$x_{ij}$对于给定的$j$全部相等时，$e_{j}$有最大值，此时:

$$p_{ij} = \frac1 m$$

$$e_j(x)|_{max}=-k\sum_{i = 1}^m \frac 1 m \ln \frac 1 m=k\ln m$$

令 $k = \frac1 {\ln m}$,则有$0<=e_{j}<=1$.

### step3:

定义差异性系数：

$g_{j} = 1-e_{j}$，该值越大，说明该指标越重要。

**熵值越小，指标的离散程度越大，该指标对综合评价的影响 (即权重) 越大。**

### step4:

定义权重：$a_{j} = g_{j}/\sum_{i = 1}^mg_{j}$

可用于评价问题的客观赋权。

### step5:

对权值排序后，从大到小可以进行累加求和，得到前$m$个成分的累计贡献率$G(m) = \sum_{j=1}^ma_{j}$,取前80%即可反应大部分指标的影响，从而达到指标剔除的效果。

## 适用范围

熵值用于赋权时，一般构建两级评价体系，上层可能需要结合专家经验来构建，而底层的指标分的比较细，权重比较难确定，这种情况下采用熵值法比较合适。

该方法没有考虑指标与指标间的相关性。

确定权重前需要确定指标对目标得分的影响方向，对非线性的指标要进行预处理或者剔除。

## 参考代码

```matlab
load shang_datas
Ind=[1 1 1 1 2]; %指定各指标的正向or负向
[S,W]=shang(X,Ind)
```

```matlab
function [s,w]=shang(x,ind)
%实现用熵值法求各指标(列）的权重及各数据行的得分
%x为原始数据矩阵, 一行代表一个样本, 每列对应一个指标
%ind指示向量，指示各列正向指标还是负向指标，1表示正向指标，2表示负向指标
%s返回各行（样本）得分，w返回各列权重
[n,m]=size(x); % n个样本, m个指标
%%数据的归一化处理
for i=1:m
    if ind(i)==1 %正向指标归一化
        X(:,i)=guiyi(x(:,i),1,0.002,0.996);    %若归一化到[0,1], 0会出问题
    else %负向指标归一化
        X(:,i)=guiyi(x(:,i),2,0.002,0.996);
    end
end
%%计算第j个指标下，第i个样本占该指标的比重p(i,j)
for i=1:n
    for j=1:m
        p(i,j)=X(i,j)/sum(X(:,j));
    end
end
%%计算第j个指标的熵值e(j)
k=1/log(n);
for j=1:m
    e(j)=-k*sum(p(:,j).*log(p(:,j)));
end
d=ones(1,m)-e; %计算信息熵冗余度
w=d./sum(d); %求权值w
s=100*w*p'; %求综合得分
```

```matlab
function y=guiyi(x,type,ymin,ymax)
%实现正向或负向指标归一化，返回归一化后的数据矩阵
%x为原始数据矩阵, 一行代表一个样本, 每列对应一个指标
%type设定正向指标1,负向指标2
%ymin,ymax为归一化的区间端点
[n,m]=size(x);
y=zeros(n,m);
xmin=min(x);
xmax=max(x);
switch type
    case 1
        for j=1:m
            y(:,j)=(ymax-ymin)*(x(:,j)-xmin(j))/(xmax(j)-xmin(j))+ymin;
        end
    case 2
        for j=1:m
            y(:,j)=(ymax-ymin)*(xmax(j)-x(:,j))/(xmax(j)-xmin(j))+ymin;
        end
end
```

## 参考

机器学习中的信息论http://saili.science/2017/09/15/entropy-method/